{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astrapi69/DroidBallet/blob/master/DLG_D2_E1_Intro_to_NNs_Exercise_Helper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='Q0'></a>\n",
        "<center><a target=\"_blank\" href=\"https://learning.constructor.org/\"><img src=\"https://drive.google.com/uc?id=1wxkbM60NlBlkbGK1JqUypKL24RrTiiYk\" width=\"200\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>\n",
        "\n",
        "<p style=\"margin-bottom:1cm;\"></p>\n",
        "\n",
        "_____\n",
        "\n",
        "<center>Constructor Learning, 2023</center>"
      ],
      "metadata": {
        "id": "QLnvSZM3xlVJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj-RvpTPhkiJ"
      },
      "source": [
        "# Exercise: Feed-forward NNs for Structured Data Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2RsfhsdhkiN"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The idea of this exercise is to continue the task of structured data classification using neural networks, starting from a raw\n",
        "CSV file. Our data includes both numerical and categorical features. You will experiment with more aspects of NN architecture in this exercise as compared to what you learnt on Day 1\n",
        "\n",
        "### The dataset\n",
        "\n",
        "[Our dataset](https://archive.ics.uci.edu/ml/datasets/heart+Disease) is provided by the\n",
        "Cleveland Clinic Foundation for Heart Disease.\n",
        "It's a CSV file with 303 rows. Each row contains information about a patient (a\n",
        "**sample**), and each column describes an attribute of the patient (a **feature**). We\n",
        "use the features to predict whether a patient has a heart disease (**binary\n",
        "classification**).\n",
        "\n",
        "Here's the description of each feature:\n",
        "\n",
        "Column| Description| Feature Type\n",
        "------------|--------------------|----------------------\n",
        "Age | Age in years | Numerical\n",
        "Sex | (1 = male; 0 = female) | Categorical\n",
        "CP | Chest pain type (0, 1, 2, 3, 4) | Categorical\n",
        "Trestbpd | Resting blood pressure (in mm Hg on admission) | Numerical\n",
        "Chol | Serum cholesterol in mg/dl | Numerical\n",
        "FBS | fasting blood sugar in 120 mg/dl (1 = true; 0 = false) | Categorical\n",
        "RestECG | Resting electrocardiogram results (0, 1, 2) | Categorical\n",
        "Thalach | Maximum heart rate achieved | Numerical\n",
        "Exang | Exercise induced angina (1 = yes; 0 = no) | Categorical\n",
        "Oldpeak | ST depression induced by exercise relative to rest | Numerical\n",
        "Slope | Slope of the peak exercise ST segment | Numerical\n",
        "CA | Number of major vessels (0-3) colored by fluoroscopy | Both numerical & categorical\n",
        "Thal | 3 = normal; 6 = fixed defect; 7 = reversible defect | Categorical\n",
        "Target | Diagnosis of heart disease (1 = true; 0 = false) | Target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Objectives - Day 2\n",
        "\n",
        "\n",
        "1. Your key task is to perform a binary classification problem to predict heart disease based on the given data by splitting it into train-valid-test datasets\n",
        "\n",
        "2. You will experiment more aspects of Dense NN models like layer activations, learning rates, regularization\n",
        "\n",
        "3. You will also plot learning curves to visualize training performance over epochs\n",
        "\n",
        "4. You will also evaluate the performance of the models on the test dataset\n",
        "\n",
        "5. You will continue using class weights to tackle class imbalance\n",
        "\n"
      ],
      "metadata": {
        "id": "4mHe52Xki94Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlZAiMhyhkiO"
      },
      "source": [
        "## Load Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOKS_8sPhkiP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(3)"
      ],
      "metadata": {
        "id": "djYfDqpaoOgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMmJ6lLVhkiQ"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "Let's download the data and load it into a Pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcWRKqt7hkiR"
      },
      "outputs": [],
      "source": [
        "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
        "df = pd.read_csv(file_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPeGuic2hkiS"
      },
      "source": [
        "The dataset includes 303 samples with 14 columns per sample (13 features, plus the target\n",
        "label):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY53Y7TOhkiT"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blfCV3iHhkiT"
      },
      "source": [
        "Here's a preview of a few samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epBGUQzWhkiU"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHQ2UgoIhkiV"
      },
      "source": [
        "The last column, \"target\", indicates whether the patient has a heart disease (1) or not\n",
        "(0)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check the distribution of the target column"
      ],
      "metadata": {
        "id": "qWKu1GeQjpkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.target.value_counts()"
      ],
      "metadata": {
        "id": "Ybl3flDXjvoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Split Dataset into Train, Validation and Test Datasets\n",
        "\n",
        "Use stratified sampling to ensure similar `target` class distribution in the dataset splits when using `train_test_split()`\n",
        "\n",
        "Use a split of 60:20:20 for train-valid-test splits\n",
        "For reproducibility set the `random_state` to `42`"
      ],
      "metadata": {
        "id": "xTyctJAqkcJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5C8ZehhEkwPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sP-nSIqpnPGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Data Pre-processing\n",
        "\n",
        "Recall the data pre-processing you had learnt during the ML week! Perform the following tasks to have clean and pre-processed datasets for your training, validation and test datasets.\n",
        "\n",
        "Key Steps:\n",
        "\n",
        "1. One-hot encode categorical data\n",
        "2. Standard Scaling numerical data\n",
        "3. Combine categorical and numeric data together into a single dataframe \\ numpy array\n",
        "\n",
        "Hints: Leverage code from [here](https://monolith.propulsion-home.ch/backend/api/momentum/materials/ds-materials/04_MachineLearning/day3/pages/MLG_D3_LC2_Classification_Models_Tackling_Class_Imbalance.html#Separate-categorical-and-numeric-columns) [pre-processing parts only]"
      ],
      "metadata": {
        "id": "N9eSMUg3HCF5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B2zlQSW-u3SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BXnWGXRj7T4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Experimenting with NN layer activation functions\n",
        "\n",
        "_[We know that our dataset is heavily imbalanced between the two classes, feel free to leverage the`class_weight` parameter in the `model.fit()` function and experiment with your own weights or use [compute class weight](https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html) from `sklearn`]_\n",
        "\n",
        "__Run the following experiments separately to train multi-layer neural networks with different activation functions__, evaluate the performance by looking at learning curve plots and finally check the performance of each of the following models on the test dataset.\n",
        "\n",
        "Key workflow:\n",
        "\n",
        "- Build model using architecture specification based on experiment\n",
        "- Plot learning curves from model training to visualize performance over epochs on training and validation data\n",
        "- Use trained model to predict and evaluate performance on test data\n",
        "- Build separate models for each experiment\n",
        "\n",
        "\n",
        "### Experiment 1 -  3-layer NN with elu activation:  \n",
        "- __3-Dense Hidden Layer, 32 units, `elu` activation function and `he_normal` as the kernel_initializer__\n",
        "- 1-Dense Output Layer, 1 unit, `sigmoid` activation function\n",
        "- __learning rate: 0.0001__\n",
        "- __optimizer is Adam__\n",
        "- metrics: 'accuracy', [precision](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision), [recall](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall)\n",
        "- loss: [binary crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy)\n",
        "- __epochs: 10000__\n",
        "- __Use the [early stopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) callback to stop training as soon as the validation loss doesn't decrease after 10 epochs i.e. `patience=10` and `monitor='val_loss'`__\n",
        "- __Modify `class_weight` in `model.fit()`__\n",
        "- batch size: 32\n",
        "- use training data and validation data in `fit()` function\n",
        "- use `predict()` on test data and show confusion matrix and classification reports\n",
        "\n",
        "<br>\n",
        "\n",
        "### Experiment 2 -  3-layer NN with leakyrelu activation:  \n",
        "- __3-Dense Hidden Layer, 32 units, LeakyReLU(alpha=0.3) activation function and `he_normal` as the kernel_initializer__\n",
        "- 1-Dense Output Layer, 1 unit, `sigmoid` activation function\n",
        "- __learning rate: 0.0001__\n",
        "- __optimizer is Adam__\n",
        "- metrics: 'accuracy', [precision](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision), [recall](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall)\n",
        "- loss: [binary crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy)\n",
        "- __epochs: 10000__\n",
        "- __Use the [early stopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) callback to stop training as soon as the validation loss doesn't decrease after 10 epochs i.e. `patience=10` and `monitor='val_loss'`__\n",
        "- __Modify `class_weight` in `model.fit()`__\n",
        "- batch size: 32\n",
        "- use training data and validation data in `fit()` function\n",
        "- use `predict()` on test data and show confusion matrix and classification reports\n",
        "\n"
      ],
      "metadata": {
        "id": "AOOGNduXHzEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hint: Utility function for plotting learning curves\n",
        "\n",
        "You can use the following function to plot the learning curves after training.\n",
        "\n",
        "Remember to do the following during training:\n",
        "\n",
        "```\n",
        "history = model.fit(.......)\n",
        "\n",
        "# to plot the curves\n",
        "plot_metrics(history)\n",
        "```"
      ],
      "metadata": {
        "id": "eUpTfVH_LR17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_metrics(history):\n",
        "\n",
        "  keys = history.history.keys()\n",
        "  metrics = ['loss', 'accuracy', 'precision', 'recall']\n",
        "  plt.figure(figsize=(12, 10))\n",
        "  for n, metric in enumerate(metrics):\n",
        "    name = metric.capitalize()\n",
        "    metric_name = [item for item in keys if metric in item and 'val_'+metric not in item][0]\n",
        "    val_metric_name = [item for item in keys if 'val_'+metric in item][0]\n",
        "\n",
        "    plt.subplot(2,2,n+1)\n",
        "    plt.plot(history.epoch, history.history[metric_name], color='b', label='Train')\n",
        "    plt.plot(history.epoch, history.history[val_metric_name], color='r',\n",
        "             linestyle=\"--\", label='Val')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(name)\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "ckAw8WMMQxkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Task 3 Below"
      ],
      "metadata": {
        "id": "EMbscmzuTv6u"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1KN6bzs9u6Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xNIo7Z-UvYQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Regularization in NNs\n",
        "\n",
        "_[We know that our dataset is heavily imbalanced between the two classes, feel free to leverage the`class_weight` parameter in the `model.fit()` function and experiment with your own weights or use [compute class weight](https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html) from `sklearn`]_\n",
        "\n",
        "__Run the following experiments separately to train multi-layer neural networks with different methods of regularization__, evaluate the performance by looking at learning curve plots and finally check the performance of each of the following models on the test dataset.\n",
        "\n",
        "We will try the following methods for regularization:\n",
        "- BatchNormalization\n",
        "- Dropout\n",
        "- Layer regularizer with L2 kernel\n",
        "\n",
        "Key workflow:\n",
        "\n",
        "- Build model using architecture specification based on experiment\n",
        "- Plot learning curves from model training to visualize performance over epochs on training and validation data\n",
        "- Use trained model to predict and evaluate performance on test data\n",
        "- Build separate models for each experiment\n",
        "\n",
        "\n",
        "### Experiment 1 -  3-layer NN with BatchNorm:  \n",
        "- __3-Dense Hidden Layer, 32 units, `relu` activation function__\n",
        "- __Add batchnorm layer after each hidden layer__\n",
        "- 1-Dense Output Layer, 1 unit, `sigmoid` activation function\n",
        "- __learning rate: 0.0001__\n",
        "- __optimizer is Adam__\n",
        "- metrics: 'accuracy', [precision](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision), [recall](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall)\n",
        "- loss: [binary crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy)\n",
        "- __epochs: 10000__\n",
        "- __Use the [early stopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) callback to stop training as soon as the validation loss doesn't decrease after 10 epochs i.e. `patience=10` and `monitor='val_loss'`__\n",
        "- __Modify `class_weight` in `model.fit()`__\n",
        "- batch size: 32\n",
        "- use training data and validation data in `fit()` function\n",
        "- use `predict()` on test data and show confusion matrix and classification reports\n",
        "\n",
        "<br>\n",
        "\n",
        "### Experiment 2 -  3-layer NN with Dropout:  \n",
        "- __3-Dense Hidden Layer, 32 units, `relu` activation function__\n",
        "- __Add dropout layers with rate = 0.2__\n",
        "- 1-Dense Output Layer, 1 unit, `sigmoid` activation function\n",
        "- __learning rate: 0.0001__\n",
        "- __optimizer is Adam__\n",
        "- metrics: 'accuracy', [precision](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision), [recall](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall)\n",
        "- loss: [binary crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy)\n",
        "- __epochs: 10000__\n",
        "- __Use the [early stopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) callback to stop training as soon as the validation loss doesn't decrease after 10 epochs i.e. `patience=10` and `monitor='val_loss'`__\n",
        "- __Modify `class_weight` in `model.fit()`__\n",
        "- batch size: 32\n",
        "- use training data and validation data in `fit()` function\n",
        "- use `predict()` on test data and show confusion matrix and classification reports\n",
        "\n",
        "<br>\n",
        "\n",
        "### Experiment 3 -  3-layer NN with L2 Layer regularizer:  \n",
        "- __3-Dense Hidden Layer, 32 units, `relu` activation function and kernel_regularizer should be `tf.keras.regularizers.l2(0.001)`__\n",
        "- 1-Dense Output Layer, 1 unit, `sigmoid` activation function\n",
        "- __learning rate: 0.0001__\n",
        "- __optimizer is Adam__\n",
        "- metrics: 'accuracy', [precision](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision), [recall](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall)\n",
        "- loss: [binary crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy)\n",
        "- __epochs: 10000__\n",
        "- __Use the [early stopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) callback to stop training as soon as the validation loss doesn't decrease after 10 epochs i.e. `patience=10` and `monitor='val_loss'`__\n",
        "- __Modify `class_weight` in `model.fit()`__\n",
        "- batch size: 32\n",
        "- use training data and validation data in `fit()` function\n",
        "- use `predict()` on test data and show confusion matrix and classification reports\n",
        "\n"
      ],
      "metadata": {
        "id": "sIRbNFHTVQha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Task 4 Below"
      ],
      "metadata": {
        "id": "MbVrrXfZWjIH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hJryC0tqw498"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lbASLykky1kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Learning Rate Scheduling in NNs\n",
        "\n",
        "_[We know that our dataset is heavily imbalanced between the two classes, feel free to leverage the`class_weight` parameter in the `model.fit()` function and experiment with your own weights or use [compute class weight](https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html) from `sklearn`]_\n",
        "\n",
        "__Run the following experiments separately to train multi-layer neural networks with different methods of learning rate schedulers__, evaluate the performance by looking at learning curve plots and finally check the performance of each of the following models on the test dataset.\n",
        "\n",
        "We will try the following methods for LR scheduling:\n",
        "- Exponential Decay\n",
        "- Cosine Decay\n",
        "\n",
        "__Recall: Learning Rate Schedulers help in increasing or decreasing learning rate with change in epochs to help coverge faster during NN training__\n",
        "\n",
        "Key workflow:\n",
        "\n",
        "- Build model using architecture specification based on experiment\n",
        "- Plot learning curves from model training to visualize performance over epochs on training and validation data\n",
        "- Use trained model to predict and evaluate performance on test data\n",
        "- Build separate models for each experiment\n",
        "\n",
        "\n",
        "### Experiment 1 -  3-layer NN with Exponential Decay LR Scheduler:  \n",
        "- __3-Dense Hidden Layer, 32 units, `relu` activation function__\n",
        "- 1-Dense Output Layer, 1 unit, `sigmoid` activation function\n",
        "- __initial learning rate: 0.001__\n",
        "- __Use [ExponentialDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) to build your own LR scheduler. Refer to [documentation](https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/) for an example__\n",
        "- __Use following config for the LR scheduler: `decay_steps=100,\n",
        "    decay_rate=0.5, staircase=True` but feel free to experiment also__\n",
        "- __optimizer is Adam__\n",
        "- metrics: 'accuracy', [precision](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision), [recall](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall)\n",
        "- loss: [binary crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy)\n",
        "- __epochs: 10000__\n",
        "- __Use the [early stopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) callback to stop training as soon as the validation loss doesn't decrease after 10 epochs i.e. `patience=10` and `monitor='val_loss'`__\n",
        "- __Modify `class_weight` in `model.fit()`__\n",
        "- batch size: 32\n",
        "- use training data and validation data in `fit()` function\n",
        "- use `predict()` on test data and show confusion matrix and classification reports\n",
        "\n",
        "<br>\n",
        "\n",
        "### Experiment 2 -  3-layer NN with Cosine Decay LR Scheduler:  \n",
        "- __3-Dense Hidden Layer, 32 units, `relu` activation function__\n",
        "- 1-Dense Output Layer, 1 unit, `sigmoid` activation function\n",
        "- __initial learning rate: 0.001__\n",
        "- __Use [CosineDecay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay) to build your own LR scheduler. Refer to [documentation](https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay/) for an example__\n",
        "- __Use following config for the LR scheduler: `decay_steps=100 but feel free to experiment also__\n",
        "- __optimizer is Adam__\n",
        "- metrics: 'accuracy', [precision](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision), [recall](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall)\n",
        "- loss: [binary crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy)\n",
        "- __epochs: 10000__\n",
        "- __Use the [early stopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) callback to stop training as soon as the validation loss doesn't decrease after 10 epochs i.e. `patience=10` and `monitor='val_loss'`__\n",
        "- __Modify `class_weight` in `model.fit()`__\n",
        "- batch size: 32\n",
        "- use training data and validation data in `fit()` function\n",
        "- use `predict()` on test data and show confusion matrix and classification reports\n",
        "\n"
      ],
      "metadata": {
        "id": "ispn2tOVXYRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Task 5 Below"
      ],
      "metadata": {
        "id": "iW_d8NDfYvjt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bnuNdx9D1Nly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KOlirWir3OiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus Task: Hyperparameter Tuning with Keras Tuner\n",
        "\n",
        "Try tuning layer hidden units and learning rate using Keras Tuner based on what you learnt during the live coding and see if you can come up with a better model.\n",
        "\n",
        "[Live coding example](https://monolith.propulsion-home.ch/backend/api/momentum/materials/ds-materials/05_DeepLearning/day2/pages/DLG_D2_LC1_Training_NN.html#Hyperparameters)"
      ],
      "metadata": {
        "id": "CYzOpEUUZDOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U keras-tuner"
      ],
      "metadata": {
        "id": "gXG7MSzJBw7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt"
      ],
      "metadata": {
        "id": "3HF99VpT_O8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0oZ2oWHCBvuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LkjYddNTCpha"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astrapi69/DroidBallet/blob/master/NLP_D1_2_LC2_Text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><a target=\"_blank\" href=\"https://learning.constructor.org/\"><img src=\"https://drive.google.com/uc?id=1RNy-ds7KWXFs7YheGo9OQwO3OnpvRSU1\" width=\"200\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>\n",
        "\n",
        "_____\n",
        "\n",
        "<center>Constructor Academy, 2024</center>"
      ],
      "metadata": {
        "id": "lkycUMJ0UGPh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l-B3avlyc8c"
      },
      "source": [
        "# Text Pre-processing\n",
        "\n",
        "Text is different than usual datasets we use to build our typical Machine Learning models. Text data needs to be pre-processed to ensure we have it in a form that is usable for various NLP tasks. Text processing and wrangling is a necessary and important step in any NLP project.\n",
        "\n",
        "In this notebook, we will cover:\n",
        "- Sentence tokenization\n",
        "- Word tokenization\n",
        "- Handling non-text characters - accents, special symbols, HTML\n",
        "- Preprocessing \\ normalization steps such as stemming, lemmatization, expanding contractions and stopword removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIaP2ot7zBGs"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEHEVTydzBsu"
      },
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import numpy as np\n",
        "import re\n",
        "import requests\n",
        "from pprint import pprint\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fppalTkNGQOW",
        "outputId": "1abf6683-5298-4c02-f9fc-f894584c688c"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgkdLpoezF0M"
      },
      "source": [
        "## Get Text Dataset\n",
        "\n",
        "Project Gutenberg is a large opensource and free collection of literary works from across the world. In this case, we will leverage content of the book **The Bible - Book 1: Genesis** for understanding different text processing and wrangling steps in the following sections\n",
        "\n",
        "\n",
        "We will also use a smaller sample text with a few short sentences to demostrate examples as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGdDEGiQzGQ3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "dbd00985-9dad-48b4-8d4d-7d31f69d3d3a"
      },
      "source": [
        "sample_text = (\"US unveils world's most powerful supercomputer, beats China. \"\n",
        "               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \"\n",
        "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
        "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
        "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
        "               \"which reportedly take up the size of two tennis courts.\")\n",
        "sample_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OiKe5_Ry29q"
      },
      "source": [
        "## Sentence Tokenization\n",
        "\n",
        "Sentence is a syntactical as well as a logical division of content in a given corpus. In order to understand text or prepare it for various tasks, understanding sentence boundaries is an important step.\n",
        "\n",
        "Sentence tokenization is the process of determining sentence boundaries in a given corpus.\n",
        "\n",
        "One might think that it is merely trivial to determine a sentence boundary, we just need to split based on \".\". While this generally holds true yet there are exceptions. Think of scenarios where we use \".\" in abbreviations and shorthand notations(such as Mr. or Mrs.).\n",
        "\n",
        "\n",
        "Let us now go through a few standard ways of performing sentence tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJDQdvp2y6Ar"
      },
      "source": [
        "### NLTK's Tokenizer\n",
        "\n",
        "NLTK provides a number of sentence tokenizers. Let's have a look at the default one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXaTV42jy6ac",
        "outputId": "53461b36-141d-42df-faf2-03bbc395b510"
      },
      "source": [
        "sample_sentences = nltk.sent_tokenize(text=sample_text)\n",
        "\n",
        "print('Total sentences in sample_text:{}'.format(len(sample_sentences)))\n",
        "print('Sample text sentences :-')\n",
        "sample_sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences in sample_text:4\n",
            "Sample text sentences :-\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"US unveils world's most powerful supercomputer, beats China.\",\n",
              " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\",\n",
              " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.',\n",
              " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spacy Tokenizer\n",
        "\n",
        "Spacy provides state-of-the-art tokenizers for tokenizing sentences"
      ],
      "metadata": {
        "id": "ugGNxGOFF49G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm') # load the english language model, supports other languages also -> check documentation"
      ],
      "metadata": {
        "id": "aLpG4mRBF_8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_spacy = nlp(sample_text)\n",
        "# get sentences\n",
        "sentences = list(text_spacy.sents)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1OeZB8oGNDv",
        "outputId": "f60e11a1-fc6e-4378-e108-5c44f4069b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[US unveils world's most powerful supercomputer, beats China.,\n",
              " The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.,\n",
              " With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.,\n",
              " Summit has 4,608 servers, which reportedly take up the size of two tennis courts.]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOiK4SLIGWtf",
        "outputId": "7581f939-c38f-45cd-c0fd-2f34cff19d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "US unveils world's most powerful supercomputer, beats China."
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(sentences[0]) # remember each sentence is not a string but a special spacy data type"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-UFAksiG4Q4",
        "outputId": "43b9181b-a840-4cf5-ffef-34c4213ea96d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.span.Span"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to access the actual string use this\n",
        "sentences[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UeoL5cDbHYvL",
        "outputId": "e672099b-2268-4dcd-bf61-37003aa8ee02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"US unveils world's most powerful supercomputer, beats China.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iT5vSgSz8C3"
      },
      "source": [
        "## Word Tokenization\n",
        "\n",
        "Word can be considered as a basic building block for NLP tasks. A combination of words make up a sentence. As in the previous section, we worked towards understanding sentence tokenization, in this section, we will focus on understanding word boundaries.\n",
        "\n",
        "We will make use of various word tokenizers available from ``nltk`` as well as ``spacy`` in this section"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK's Tokenizer\n",
        "\n",
        "NLTK provides a number of word tokenizers. Let's have a look at the default one."
      ],
      "metadata": {
        "id": "qbe_xmXZH_0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "LMbDfYiUfinQ",
        "outputId": "1ff1612f-4b84-4021-8479-16ce4788dfac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoL-Yo18z932",
        "outputId": "cb07f682-06e4-48c4-ee01-90a3ca611859"
      },
      "source": [
        "# default tokenizer\n",
        "words = nltk.word_tokenize(sample_text)\n",
        "np.array(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the',\n",
              "       'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight',\n",
              "       '.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
              "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
              "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
              "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
              "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608',\n",
              "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
              "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZmp-xdsKJo3"
      },
      "source": [
        "### Spacy Tokenizer\n",
        "\n",
        "Spacy provides state-of-the-art tokenizers for tokenizing words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm') # shown just to remind you, no need to do this more than once\n",
        "text_spacy = nlp(sample_text)"
      ],
      "metadata": {
        "id": "pzIdzLjvbBXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [word.text for word in text_spacy]  #word tokenization\n",
        "np.array(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DFZu8v5HspR",
        "outputId": "f8585eee-0383-434b-e20c-84e9934c87fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
              "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
              "       'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', 'beating',\n",
              "       'the', 'previous', 'record', '-', 'holder', 'China', \"'s\",\n",
              "       'Sunway', 'TaihuLight', '.', 'With', 'a', 'peak', 'performance',\n",
              "       'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',',\n",
              "       'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway',\n",
              "       'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000',\n",
              "       'trillion', 'calculations', 'per', 'second', '.', 'Summit', 'has',\n",
              "       '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up',\n",
              "       'the', 'size', 'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-qnbtAZ0VAE"
      },
      "source": [
        "## Handling Non-Text Characters\n",
        "\n",
        "Natural text consists of various types of characters such as alphabets, numbers, symbols, emoticons, non-printable characters and so on. For most practical use-cases we limit ourselves to alphabets (at most numbers) and ignore other types of characters.\n",
        "\n",
        "In this section, we will focus on identification of non-text characters and how to remove them from our corpus safely.\n",
        "\n",
        "We will focus on handling following types of characters:\n",
        "- Accented Characters\n",
        "- Special Characters\n",
        "- HTML Tags & Noise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzFbsU1e0put"
      },
      "source": [
        "### Accented Characters\n",
        "\n",
        "The most common accents are the acute (é), grave (è), circumflex (â, î or ô), tilde (ñ), umlaut and dieresis (ü or ï – the same symbol is used for two different purposes), and cedilla (ç). Accent marks (also referred to as diacritics or diacriticals) usually appear above a character.\n",
        "\n",
        "These characters are part of extended alphabet in languages such as French, Spanish, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QRMw2Za0VXL"
      },
      "source": [
        "import unicodedata\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "guR7PRqX0zEF",
        "outputId": "bfc45337-36c6-430b-99ec-206b10a3db40"
      },
      "source": [
        "s = 'Sómě Áccěntěd těxt'\n",
        "s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sómě Áccěntěd těxt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GipXUT6yLCEy",
        "outputId": "0c203fbd-d7ac-4d51-8722-a05257afbb7b"
      },
      "source": [
        "remove_accented_chars(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Some Accented text'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w0miO_i0ewB"
      },
      "source": [
        "### Special Characters\n",
        "\n",
        "Symbols, emoticons and characters such as ``#``, ``@`` etc. are considered special characters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [^a-zA-Z0-9\\s] => this will remove anything which is not a letter (eng alphabet), number or space"
      ],
      "metadata": {
        "id": "V-E3O1uLYw_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rQSOYM40fGG"
      },
      "source": [
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
        "    print('Pattern used is:', pattern)\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rRR5f_Vk1OqQ",
        "outputId": "7c541430-5ba4-4b56-bab7-32a3ce535446"
      },
      "source": [
        "s = \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂\"\n",
        "s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "HL0DRSHYK_Jv",
        "outputId": "1a4a142c-0000-4b5b-d7b9-721718cd78d5"
      },
      "source": [
        "remove_special_characters(s, remove_digits=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pattern used is: [^a-zA-Z\\s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Well this was fun See you at  What do you think  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "dz-h-z9K1Vk8",
        "outputId": "b2b085a2-f78a-4c3d-d117-35e49e9c1d3d"
      },
      "source": [
        "remove_special_characters(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pattern used is: [^a-zA-Z0-9\\s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Well this was fun See you at 730 What do you think 9318 '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GG1e7Xh1pBJ"
      },
      "source": [
        "### HTML Tags & Noise\n",
        "\n",
        "Many times, NLP datasets are collected as part of web-scraping activities. Web-scraping involves scanning various websites to extract text from them. This process leads to content which is a mix of actual text as well as HTML tags.\n",
        "\n",
        "In this section we will extract HTML version of ** The Bible** book. We will then use ``BeautifulSoup`` to clean out HTML tags to get actual text."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bible_html_url = 'http://www.gutenberg.org/cache/epub/8001/pg8001.html'"
      ],
      "metadata": {
        "id": "cuZWEyd_ZX8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcCctZEg1pV2",
        "outputId": "7bcabc6c-b8af-4593-be75-8e7d13d7c8d0"
      },
      "source": [
        "data = requests.get(bible_html_url)\n",
        "content = data.text\n",
        "print(content[7700:9000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\" style=\"margin-top: 5em\">Book 01        Genesis</h1>\r\n",
            "\r\n",
            "<p id=\"id00003\">01:001:001 In the beginning God created the heaven and the earth.</p>\r\n",
            "\r\n",
            "<p id=\"id00004\" style=\"margin-left: 0%; margin-right: 0%\">01:001:002 And the earth was without form, and void; and darkness was\r\n",
            "           upon the face of the deep. And the Spirit of God moved upon\r\n",
            "           the face of the waters.</p>\r\n",
            "\r\n",
            "<p id=\"id00005\">01:001:003 And God said, Let there be light: and there was light.</p>\r\n",
            "\r\n",
            "<p id=\"id00006\">01:001:004 And God saw the light, that it was good: and God divided the<br>\r\n",
            "\r\n",
            "           light from the darkness.<br>\r\n",
            "</p>\r\n",
            "\r\n",
            "<p id=\"id00007\">01:001:005 And God called the light Day, and the darkness he called<br>\r\n",
            "\r\n",
            "           Night. And the evening and the morning were the first day.<br>\r\n",
            "</p>\r\n",
            "\r\n",
            "<p id=\"id00008\">01:001:006 And God said, Let there be a firmament in the midst of the<br>\r\n",
            "\r\n",
            "           waters, and let it divide the waters from the waters.<br>\r\n",
            "</p>\r\n",
            "\r\n",
            "<p id=\"id00009\" style=\"margin-left: 0%; margin-right: 0%\">01:001:007 And God made the firmament, and divided the waters which were\r\n",
            "           under the firmament from the waters which were above the\r\n",
            "           firmament: and it was so.</p>\r\n",
            "\r\n",
            "<p id=\"id00010\" style=\"margin-left: 0%; margin-right: 0%\">01:001:008 And God called the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ5sHUtvKsZz"
      },
      "source": [
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])] # basically reject all script tags in HTML\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text) # remove extra blank newlines and replace with single newlines\n",
        "    return stripped_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJiDezzJKygr",
        "outputId": "a5fb296c-2184-4670-9c36-69cc77b03557"
      },
      "source": [
        "clean_content = strip_html_tags(content)\n",
        "print(clean_content[1036:2000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Book 01        Genesis\n",
            "01:001:001 In the beginning God created the heaven and the earth.\n",
            "01:001:002 And the earth was without form, and void; and darkness was\n",
            "           upon the face of the deep. And the Spirit of God moved upon\n",
            "           the face of the waters.\n",
            "01:001:003 And God said, Let there be light: and there was light.\n",
            "01:001:004 And God saw the light, that it was good: and God divided the\n",
            "           light from the darkness.\n",
            "01:001:005 And God called the light Day, and the darkness he called\n",
            "           Night. And the evening and the morning were the first day.\n",
            "01:001:006 And God said, Let there be a firmament in the midst of the\n",
            "           waters, and let it divide the waters from the waters.\n",
            "01:001:007 And God made the firmament, and divided the waters which were\n",
            "           under the firmament from the waters which were above the\n",
            "           firmament: and it was so.\n",
            "01:001:008 And God called the firmament Heaven. And the evening and the\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LxLCnCFxlpf"
      },
      "source": [
        "\n",
        "That seemed to have worked like a charm!\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEegW2cg0yiZ"
      },
      "source": [
        "## Text Normalization\n",
        "\n",
        "In this section, we will prepare utilities to fix different issues with textual data.\n",
        "\n",
        "- Expand Contractions\n",
        "- Stemming\n",
        "- Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e_cotz81RKq"
      },
      "source": [
        "## Stemming\n",
        "\n",
        "In linguistic morphology and information retrieval, stemming is the process of reducing inflected words to their word stem, base or root form—generally a written word form"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwjTfalN0U8t"
      },
      "source": [
        "#### Porter Stemmer\n",
        "\n",
        "The Porter stemming algorithm (or 'Porter stemmer') is a process for removing the commoner morphological and inflexional endings from words in English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSphmzxS1Slq"
      },
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znVyq3LLPQh_",
        "outputId": "6e227591-f38b-4eec-87bd-f892194f3199"
      },
      "source": [
        "ps = PorterStemmer()\n",
        "\n",
        "ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('jump', 'jump', 'jump')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PcKf-pNkQGKy",
        "outputId": "30ebe837-df22-4183-9550-a86f2fd1fbd2"
      },
      "source": [
        "ps.stem('lying')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lie'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Wn-IMSgCQF5E",
        "outputId": "27cdc993-d3ff-41c7-f043-2b9f5866ab7d"
      },
      "source": [
        "ps.stem('strange')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'strang'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIgS4XM00nak"
      },
      "source": [
        "#### Lancaster Stemmer\n",
        "The Lancaster stemmers are more aggressive and dynamic compared to the other two stemmers. The stemmer is really faster, but the algorithm is really confusing when dealing with small words. But they are not as efficient as Snowball Stemmers. The Lancaster stemmers save the rules externally and basically uses an iterative algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc0Mas_kQObP",
        "outputId": "6af4e0eb-45f7-405b-e516-1ca3b9c23809"
      },
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "ls = LancasterStemmer()\n",
        "\n",
        "ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('jump', 'jump', 'jump')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IyYoJWvuQPYO",
        "outputId": "49fd0477-919f-44ac-9029-cf696c793459"
      },
      "source": [
        "ls.stem('lying')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lying'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-_t_nRSoQPO1",
        "outputId": "e90aadee-9579-4348-b769-22d6e9de31ec"
      },
      "source": [
        "ls.stem('strange')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'strange'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt9y8f0ZQWAb"
      },
      "source": [
        "ps = nltk.porter.PorterStemmer()\n",
        "ls = nltk.stem.LancasterStemmer()\n",
        "\n",
        "def simple_stemmer(text, stemmer=ps):\n",
        "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "    # alternate way\n",
        "    # words = nltk.word_tokenize(text)\n",
        "    # text = ' '.join(words)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQBpxXCb4eGh"
      },
      "source": [
        "#### Try calling the above defined function for both Lancaster and Porter stemmer separately"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-RK-0_Pu4rmm",
        "outputId": "fc4f28dc-aa02-4e39-fe83-5d82be7bee5b"
      },
      "source": [
        "s = \"My system keeps crashing his crashed yesterday ours crashes daily and presumably we are not lying\"\n",
        "s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'My system keeps crashing his crashed yesterday ours crashes daily and presumably we are not lying'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "A-gWD0o5QVnp",
        "outputId": "7cbcfd58-8aaa-4af3-a1b8-19aba7fb20c8"
      },
      "source": [
        "simple_stemmer(s, stemmer=ps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'my system keep crash hi crash yesterday our crash daili and presum we are not lie'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mHM0-NyF4xgi",
        "outputId": "fb65f778-8e89-4298-9a4d-d5b254f18fe5"
      },
      "source": [
        "simple_stemmer(s, stemmer=ls)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'my system keep crash his crash yesterday our crash dai and presum we ar not lying'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To ensure grammatically correct words use lemmatization"
      ],
      "metadata": {
        "id": "OoOZ1VvnLdjy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh2VFpaT1TKx"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jAS_a-cu6Obm",
        "outputId": "9fd7f29d-1743-4d85-8b11-234744b88efd"
      },
      "source": [
        "s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'\n",
        "s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LaWjzXYQzFM"
      },
      "source": [
        "#### Spacy Lemmatization\n",
        "\n",
        "Out of the box implementation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[word.lemma_ for word in nlp(s)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNh6pHqqeD8z",
        "outputId": "fe779175-03a9-4354-e939-f2b2b1191b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'brown',\n",
              " 'fox',\n",
              " 'be',\n",
              " 'quick',\n",
              " 'and',\n",
              " 'they',\n",
              " 'be',\n",
              " 'jump',\n",
              " 'over',\n",
              " 'the',\n",
              " 'sleep',\n",
              " 'lazy',\n",
              " 'dog',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8KMWjKtQwEd"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_\n",
        "                          for word in text])\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PdTC1fcTBwPR",
        "outputId": "d02a5019-e457-4258-d96b-f4e86ecae504"
      },
      "source": [
        "s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qGhiUFpFQ4B_",
        "outputId": "7e07ae9b-012c-4cba-fa7d-d27cf6e64e48"
      },
      "source": [
        "lemmatize_text(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the brown fox be quick and they be jump over the sleep lazy dog !'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63RQADG9RCDT"
      },
      "source": [
        "## Stopword Removal\n",
        "\n",
        "In computing, stop words are words which are filtered out before or after processing of natural language data. A stop word is a commonly used word (such as “the”, “a”, “an”,etc.) which does not convey a lot of useful information\n",
        "\n",
        "We typically remove stopwords before using text for most NLP tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0YRm3Kncy3n",
        "outputId": "d8011171-8789-493f-8f33-f21f07261698"
      },
      "source": [
        "nltk.corpus.stopwords.words('english')  # standard set of stopwords - words with not much meaning"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZCYbpMrQ9GR"
      },
      "source": [
        "def remove_stopwords(text, stopwords=None):\n",
        "    if not stopwords:\n",
        "        stopwords = nltk.corpus.stopwords.words('english')\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens] # removing any whitespaces from words\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVfF6BUY7AgI",
        "outputId": "00a8b2a7-fe3d-40a5-c15a-69b19020f1e3"
      },
      "source": [
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "print(stop_words[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bo2AVwoARIm3",
        "outputId": "db531858-6546-4a39-a704-30a742659620"
      },
      "source": [
        "s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BrZP5DCp7D1c",
        "outputId": "47b3cb55-8e28-44eb-b223-e146d4fe4ec2"
      },
      "source": [
        "remove_stopwords(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'brown foxes quick jumping sleeping lazy dogs !'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3-IWN2s7I2o"
      },
      "source": [
        "You can customize your own list of stopwords if needed.\n",
        "\n",
        "Example:\n",
        "\n",
        "Remove the word 'the' and add the word 'brown' from the stop_words list and call the function with this new list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHmcJUHWf6NG",
        "outputId": "f2127e7b-f6ce-46b2-9b59-af9d3231a855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiLGzqCG7Jt2"
      },
      "source": [
        "stop_words.remove('the')\n",
        "stop_words.append('brown')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DbiyGZEm7LRz",
        "outputId": "e0307434-8310-4ce0-b669-6d8be256ba58"
      },
      "source": [
        "remove_stopwords(s, stopwords=stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The foxes quick jumping the sleeping lazy dogs !'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9NJ07FQRXWO"
      },
      "source": [
        "## Expand Contractions\n",
        "\n",
        "Contractions are words or combinations of words that are shortened by dropping letters and replacing them by an apostrophe.\n",
        "\n",
        "In order to capture context better, we standardize text by expanding such contractions. ``contractions`` and ``textsearch`` enable us to do so in just a few lines of code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWUKQuIhROAt",
        "outputId": "0fee28d5-21e5-4066-c67d-952258c9b127"
      },
      "source": [
        "!pip install contractions\n",
        "!pip install textsearch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.10/dist-packages (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qyYeGwhRlPL"
      },
      "source": [
        "import contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc41Ls6KRglA",
        "outputId": "834c73fb-cf12-4533-c9cc-9320484b9904"
      },
      "source": [
        "list(contractions.contractions_dict.items())[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"I'm\", 'I am'),\n",
              " (\"I'm'a\", 'I am about to'),\n",
              " (\"I'm'o\", 'I am going to'),\n",
              " (\"I've\", 'I have'),\n",
              " (\"I'll\", 'I will'),\n",
              " (\"I'll've\", 'I will have'),\n",
              " (\"I'd\", 'I would'),\n",
              " (\"I'd've\", 'I would have'),\n",
              " ('Whatcha', 'What are you'),\n",
              " (\"amn't\", 'am not')]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V-aX01iRcmW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3a90da3d-0a24-4205-dc3e-2017bb880590"
      },
      "source": [
        "sample_string = \"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\"\n",
        "sample_string"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hNR1iYWlRoVc",
        "outputId": "cdcc77d4-1f7b-49fc-d105-a3c6f86ef03b"
      },
      "source": [
        "contractions.fix(sample_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You all cannot expand contractions I would think! You would not be able to. How did you do it?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    }
  ]
}
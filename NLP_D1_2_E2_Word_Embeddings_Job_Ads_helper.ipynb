{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "NLP",
      "language": "python",
      "name": "nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astrapi69/DroidBallet/blob/master/NLP_D1_2_E2_Word_Embeddings_Job_Ads_helper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma8sSG9cX4o-"
      },
      "source": [
        "<a id='Q0'></a>\n",
        "<center><a target=\"_blank\" href=\"https://learning.constructor.org/\"><img src=\"https://drive.google.com/uc?id=1RNy-ds7KWXFs7YheGo9OQwO3OnpvRSU1\" width=\"200\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>\n",
        "\n",
        "_____\n",
        "\n",
        "<center> <h1> Helper Notebook: Projecting Word Embeddings </h1> </center>\n",
        "\n",
        "<p style=\"margin-bottom:1cm;\"></p>\n",
        "\n",
        "_____\n",
        "\n",
        "<center>Constructor Academy, 2024</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r94c3U9cX4pH"
      },
      "source": [
        "# Helper Notebook: Projecting Word Embeddings\n",
        "We will work with job ads from job.ch. A dataset of 10000 English job ads is provided.\n",
        "\n",
        "The goal of this exercise will be to develop a working understanding of Word2vec and use t-sne as a way to analyze word embeddings\n",
        "\n",
        "Like any classical NLP task the steps in this analysis will be\n",
        "\n",
        "- Clean data\n",
        "- Build a corpus\n",
        "- Train word2vec\n",
        "- Visualize using t-sne"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install umap-learn"
      ],
      "metadata": {
        "id": "wkY5HXJWmxGe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c74f2a1-5247-4334-f31a-647c1664c0ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 KB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from umap-learn) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.9/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.9/dist-packages (from umap-learn) (1.10.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.9/dist-packages (from umap-learn) (0.56.4)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.8.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from umap-learn) (4.65.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.49->umap-learn) (67.6.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.49->umap-learn) (0.39.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.9/dist-packages (from pynndescent>=0.5->umap-learn) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82830 sha256=3c732b18552b50479101dfc7d7bfab4e47af2185d5f36899b673e2f1fa2432f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/3e/1c/596d0a463d17475af648688443fa4846fef624d1390339e7e9\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.8-py3-none-any.whl size=55509 sha256=064e3f365798acd9f486b04ca9b796889723b494de284421c0cb402cd0115176\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/89/cc/59ab91ef5b21dc2ab3635528d7d227f49dfc9169905dcb959d\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.8 umap-learn-0.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-07T16:56:09.469904Z",
          "start_time": "2019-01-07T16:56:07.858398Z"
        },
        "id": "QDc6DL6uX4pI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac15e57e-a34e-45d2-edff-93d9df2b4be8"
      },
      "source": [
        "import re\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import umap\n",
        "from gensim.models import word2vec\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMP5geXzceU4"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLZhhtHRbmPq"
      },
      "source": [
        "!curl -L -o job_ads_eng.csv \"http://drive.google.com/uc?export=download&id=1IGCgrq7AqygIaLcjiFwlqgcNoQd1OAqo\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvpOIkLbX4pJ"
      },
      "source": [
        "## Data preparation\n",
        "### Load the data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-07T16:56:09.469904Z",
          "start_time": "2019-01-07T16:56:07.858398Z"
        },
        "id": "hWn2zmL5X4pJ"
      },
      "source": [
        "data = pd.read_csv(\"job_ads_eng.csv\")  # .sample(50000, random_state=23)\n",
        "data.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5b07A7QX4pL"
      },
      "source": [
        "### Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-07T16:57:08.183346Z",
          "start_time": "2019-01-07T16:56:11.334721Z"
        },
        "id": "FBoh6y6kX4pM"
      },
      "source": [
        "STOP_WORDS = nltk.corpus.stopwords.words(\"english\")\n",
        "\n",
        "\n",
        "def clean_sentence(sentence):\n",
        "    \"\"\"\n",
        "    remove chars that are not letters or numbers, downcase\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    # Hint: Use regex!\n",
        "\n",
        "\n",
        "def remove_stopwords(sentence):\n",
        "    \"\"\"\n",
        "    remove stopwords\n",
        "    \"\"\"\n",
        "    # TODO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-07T16:57:08.183346Z",
          "start_time": "2019-01-07T16:56:11.334721Z"
        },
        "id": "aRp1NbLJX4pM"
      },
      "source": [
        "data = data.dropna(subset=[\"Content\"])  # remove rows without content\n",
        "data[\"Content\"] = data[\"Content\"].apply(clean_sentence)\n",
        "data[\"Content\"] = data[\"Content\"].apply(remove_stopwords)\n",
        "data.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIbG6Hw-X4pN"
      },
      "source": [
        "# Let's have a look at an example text\n",
        "data[data[\"Job title\"].str.contains(\"Data\")][\"Content\"].values[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrZMBFKUX4pN"
      },
      "source": [
        "### Create the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayABbtBcX4pO"
      },
      "source": [
        "# Create a list of lists containing the words of each description\n",
        "\n",
        "# TODO\n",
        "corpus = ??\n",
        "corpus[0][:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwObUiFSX4pO"
      },
      "source": [
        "## Create word embeddings\n",
        "We use word2vec of the gensim package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrhoGpmUX4pP"
      },
      "source": [
        "# TODO\n",
        "model = ??\n",
        "model.wv[\"email\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqWiMmmdX4pP"
      },
      "source": [
        "## Project and plot embeddings\n",
        "Let's use t-SNE or umap to project the embeddings into a 2 or 3-dim space. For plotting we use an interactive plotly plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUVjsRdEX4pP"
      },
      "source": [
        "from plotly import express as px\n",
        "\n",
        "\n",
        "def plot_embeddings(model, projection=\"tsne\", dim=2, wordlist=None, **kwargs):\n",
        "\n",
        "    vectors_proj, lables = project_embeddings(\n",
        "        model, projection=projection, dim=dim, wordlist=wordlist, **kwargs\n",
        "    )\n",
        "\n",
        "    if dim == 2:\n",
        "        plot_2d(vectors_proj, lables)\n",
        "    elif dim == 3:\n",
        "        plot_3d(vectors_proj, lables)\n",
        "    else:\n",
        "        raise ValueError(\"Dimension of input vectors has to be 2 or 3.\")\n",
        "\n",
        "\n",
        "def project_embeddings(model, projection=\"tsne\", dim=2, wordlist=None, **kwargs):\n",
        "    if not wordlist:\n",
        "        wordlist = model.wv.key_to_index\n",
        "\n",
        "    lables = [word for word in wordlist]\n",
        "    vectors = [model.wv[word] for word in wordlist]\n",
        "\n",
        "    if projection == \"tsne\":\n",
        "        vectors_proj = call_tsne(vectors, n_components=dim, **kwargs)\n",
        "    elif projection == \"umap\":\n",
        "        vectors_proj = call_umap(vectors, n_components=dim, **kwargs)\n",
        "    return vectors_proj, lables\n",
        "\n",
        "\n",
        "def call_tsne(vectors, n_components, **kwargs):\n",
        "    arguments = dict(perplexity=40, init=\"pca\", n_iter=2500, random_state=23)\n",
        "    arguments.update(kwargs)\n",
        "    tsne_model = TSNE(n_components=n_components, **arguments)\n",
        "    vectors_proj = tsne_model.fit_transform(vectors)\n",
        "    return vectors_proj\n",
        "\n",
        "\n",
        "def call_umap(vectors, n_components, **kwargs):\n",
        "    arguments = dict(n_neighbors=15, min_dist=0.1, metric=\"euclidean\")\n",
        "    arguments.update(kwargs)\n",
        "    umap_model = umap.UMAP(random_state=42, n_components=n_components, **arguments)\n",
        "    vectors_proj = umap_model.fit_transform(vectors)\n",
        "    return vectors_proj\n",
        "\n",
        "\n",
        "def plot_2d(vectors_proj, lables=None):\n",
        "    x = [vec[0] for vec in vectors_proj]\n",
        "    y = [vec[1] for vec in vectors_proj]\n",
        "\n",
        "    fig = px.scatter(x=x, y=y, text=lables)\n",
        "    fig.update_traces(textposition=\"top center\", textfont_size=10)\n",
        "    fig.update_layout(height=800, title_text=\"2d projection of word embeddings\")\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "def plot_3d(vectors_proj, lables=None):\n",
        "    x = [vec[0] for vec in vectors_proj]\n",
        "    y = [vec[1] for vec in vectors_proj]\n",
        "    z = [vec[2] for vec in vectors_proj]\n",
        "\n",
        "    fig = px.scatter_3d(x=x, y=y, z=z, text=lables)\n",
        "    fig.update_traces(textposition=\"top center\", textfont_size=10, marker_size=3)\n",
        "    fig.update_layout(height=800, title_text=\"3d projection of word embeddings\")\n",
        "    fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGJ42ntZX4pQ"
      },
      "source": [
        "plot_embeddings(model, projection=\"umap\", dim=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7ETfmB7X4pQ"
      },
      "source": [
        "## Export the word embeddings\n",
        "This allows us to visualize them at https://projector.tensorflow.org/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O1MSh9QX4pR"
      },
      "source": [
        "![tensorflow_projector_job_adds.gif](attachment:tensorflow_projector_job_adds.gif)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViJok8txX4pR"
      },
      "source": [
        "# TODO\n",
        "lables = ??\n",
        "vectors = ??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TNhJmTgX4pR"
      },
      "source": [
        "pd.DataFrame(lables).to_csv(\"lables.tsv\", sep=\"\\t\", index=False, header=False)\n",
        "pd.DataFrame(vectors).to_csv(\"vectors.tsv\", sep=\"\\t\", index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7VljjhdX4pS"
      },
      "source": [
        "Go to https://projector.tensorflow.org/. Click `Load` for uploading the vectors and the labels files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne7u3fnrX4pS"
      },
      "source": [
        "## Word embeddings - Try different parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzMngrtOX4pS"
      },
      "source": [
        "# A more selective model, word has to be at least 1000 times in the corpus\n",
        "\n",
        "# TODO\n",
        "model = ??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN4R3nXVX4pS"
      },
      "source": [
        "plot_embeddings(model, projection=\"umap\", dim=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ES_3RBpX4pT"
      },
      "source": [
        "# Creat word embeddings with 300 components\n",
        "\n",
        "# TODO\n",
        "model = ??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "239u1UI0X4pT"
      },
      "source": [
        "plot_embeddings(model, projection=\"umap\", dim=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQoNkisqX4pU"
      },
      "source": [
        "## Let's find some similar words to our query\n",
        "Create the word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY9QVatdX4pU"
      },
      "source": [
        "# TODO\n",
        "model = ??"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0inNQb4X4pU"
      },
      "source": [
        "Define a search word and find the most similar words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC92dnlKX4pU"
      },
      "source": [
        "# plot the most similiar words\n",
        "search_word = \"python\"\n",
        "\n",
        "# TODO\n",
        "m_similar = ??\n",
        "wordlist = ??\n",
        "# add the word itself\n",
        "wordlist.append(search_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdq23JIkX4pV"
      },
      "source": [
        "Plot the search word together with the similar words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OII1QK4pX4pW"
      },
      "source": [
        "plot_embeddings(model, projection=\"umap\", dim=2, wordlist=wordlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkk3PuuYX4pW"
      },
      "source": [
        "Print the list of similar words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqPyCNR9X4pX"
      },
      "source": [
        "m_similar"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
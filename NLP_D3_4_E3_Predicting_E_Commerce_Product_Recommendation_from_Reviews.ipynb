{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astrapi69/DroidBallet/blob/master/NLP_D3_4_E3_Predicting_E_Commerce_Product_Recommendation_from_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><a target=\"_blank\" href=\"https://learning.constructor.org/\"><img src=\"https://drive.google.com/uc?id=1wxkbM60NlBlkbGK1JqUypKL24RrTiiYk\" width=\"200\" style=\"background:none; border:none; box-shadow:none;\" /></a> </center>\n",
        "\n",
        "_____\n",
        "\n",
        "<center>Constructor Learning, 2023</center>"
      ],
      "metadata": {
        "id": "luYObrywv246"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLjPUgZ67ghr"
      },
      "source": [
        "# Predicting E-Commerce Product Recommendations from Reviews\n",
        "\n",
        "\n",
        "![](https://github.com/dipanjanS/feature_engineering_session_dhs18/blob/master/ecommerce_product_ratings_prediction/clothing_banner.jpg?raw=1)\n",
        "\n",
        "This is a classic NLP problem dealing with data from an e-commerce store focusing on women's clothing. Each record in the dataset is a customer review which consists of the review title, text description and a recommendation 0 or 1) for a product amongst other features\n",
        "\n",
        "\n",
        "__Main Objective:__ Leverage the review text attributes and build deep learning models to predict the recommendation (classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_-DF7GU7ghz"
      },
      "source": [
        "# Load up basic dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70fh2k6W7gh0"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sYue8WVCS6g"
      },
      "source": [
        "!pip install contractions\n",
        "!pip install textsearch\n",
        "!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It-AoJnd7gh1"
      },
      "source": [
        "# Load and View the Dataset\n",
        "\n",
        "The data is available at https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews from where you can download it.\n",
        "\n",
        "You can also access it from my [__GitHub Repo__](https://github.com/dipanjanS/text-analytics-with-python/blob/master/media) if needed.\n",
        "\n",
        "Following code enables it to get it easily from the web."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pLTPQ5Jz7gh2"
      },
      "source": [
        "df = pd.read_csv('https://github.com/dipanjanS/text-analytics-with-python/raw/master/media/Womens%20Clothing%20E-Commerce%20Reviews%20-%20NLP.csv', keep_default_na=False)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9woiRgU7gh3"
      },
      "source": [
        "# Basic Data Processing\n",
        "\n",
        "- Merge all review text attributes (title, text description) into one attribute\n",
        "- Subset out columns of interest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVlIgJhz7gh4"
      },
      "source": [
        "df['Review'] = (df['Title'].map(str) +' '+ df['Review Text']).apply(lambda row: row.strip())\n",
        "df['Recommended'] = df['Recommended IND']\n",
        "df = df[['Review', 'Recommended']]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmRclPlP7gh4"
      },
      "source": [
        "## Remove all records with no review text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myWq442E7gh5"
      },
      "source": [
        "df = df[df['Review'] != '']\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3DtNfBN7gh5"
      },
      "source": [
        "## There is some imbalance in the data based on product recommendations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFZRMQuw7gh6"
      },
      "source": [
        "df['Recommended'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtwUKJRz7gh6"
      },
      "source": [
        "# Build train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18tjQTzp7gh7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['Recommended']), df['Recommended'], test_size=0.3, random_state=42)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAZ3usVY7gh7"
      },
      "source": [
        "from collections import Counter\n",
        "Counter(y_train), Counter(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OlGfG-o-Ce8"
      },
      "source": [
        "X_train.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9kZRt4M-FM9"
      },
      "source": [
        "y_train[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSwuHSn57giH"
      },
      "source": [
        "# Text Pre-processing and Wrangling\n",
        "\n",
        "We do minimal text pre-processing here given we will be building deep learning models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDR7QMBw7giH"
      },
      "source": [
        "import nltk\n",
        "import contractions\n",
        "import re\n",
        "import tqdm\n",
        "\n",
        "\n",
        "def normalize_document(doc):\n",
        "    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    doc = doc.lower()\n",
        "    doc = contractions.fix(doc)\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', ' ', doc, re.I|re.A)\n",
        "    doc = re.sub(' +', ' ', doc)\n",
        "    doc = doc.strip()\n",
        "\n",
        "    return doc\n",
        "\n",
        "def normalize_corpus(docs):\n",
        "    norm_docs = []\n",
        "    for doc in tqdm.tqdm(docs):\n",
        "        norm_doc = normalize_document(doc)\n",
        "        norm_docs.append(norm_doc)\n",
        "\n",
        "    return norm_docs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHtJdSHp7giH"
      },
      "source": [
        "X_train['Clean Review'] = normalize_corpus(X_train['Review'].values)\n",
        "X_test['Clean Review'] = normalize_corpus(X_test['Review'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D40WbQTE7giI"
      },
      "source": [
        "# Experiment 1: Train Classfier with CNN + FastText Embeddings & Evaluate Performance on Test Data\n",
        "\n",
        "__Note:__ Skip FastText Embeddings part if it takes too much time to download or load it since it does consume a good amount of memory to load the pretrained embeddings.\n",
        "\n",
        "If you want to load pre-trained embeddings use a slightly smaller file than the one we used in live-coding which had over 2 million words. Here is the link to get embeddings from facebook's pre-trained fasttext model.\n",
        "\n",
        "https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "\n",
        "__Hint:__ Use the code from the live-coding session to download and load relevant embeddings from the above dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT0Op1yXnLXg"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rgrhj7b7giI"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMaKgLjwnaiL"
      },
      "source": [
        "# Experiment 2: Train Classfier with LSTM + FastText Embeddings & Evaluate Performance on Test Data\n",
        "\n",
        "__Note:__ Skip FastText Embeddings part if it takes too much time to download or load it since it does consume a good amount of memory to load the pretrained embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1HOmYZ_-s1L"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvHdeJZX-s3F"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg1IK8h_ndwF"
      },
      "source": [
        "# Experiment 3: Train Classfier with NNLM Universal Embedding Model\n",
        "\n",
        "__Hint:__ This model should accept the pre-processed text directly (as shown in livecoding)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOOD_ZNqnkOY"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3xwj2D1Cuo7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDEbMGtDntDf"
      },
      "source": [
        "# Experiment 4: Train Classfier with BERT\n",
        "\n",
        "##### Note: You might need to restart the notebook environment on colab after installing the below library\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HDyoYcCFy7X"
      },
      "source": [
        "!pip install transformers --ignore-installed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvjmoKn8n9WT"
      },
      "source": [
        "##### Note: Run the below cell to get all the pre-processed data again in case you needed to reload the notebook after the above installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1VKGDpQGG-Z"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('https://github.com/dipanjanS/text-analytics-with-python/raw/master/media/Womens%20Clothing%20E-Commerce%20Reviews%20-%20NLP.csv', keep_default_na=False)\n",
        "df['Review'] = (df['Title'].map(str) +' '+ df['Review Text']).apply(lambda row: row.strip())\n",
        "df['Recommended'] = df['Recommended IND']\n",
        "df = df[['Review', 'Recommended']]\n",
        "df = df[df['Review'] != '']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['Recommended']), df['Recommended'], test_size=0.3, random_state=42)\n",
        "\n",
        "import nltk\n",
        "import contractions\n",
        "import re\n",
        "import tqdm\n",
        "\n",
        "\n",
        "def normalize_document(doc):\n",
        "    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    doc = doc.lower()\n",
        "    doc = contractions.fix(doc)\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', ' ', doc, re.I|re.A)\n",
        "    doc = re.sub(' +', ' ', doc)\n",
        "    doc = doc.strip()\n",
        "\n",
        "    return doc\n",
        "\n",
        "def normalize_corpus(docs):\n",
        "    norm_docs = []\n",
        "    for doc in tqdm.tqdm(docs):\n",
        "        norm_doc = normalize_document(doc)\n",
        "        norm_docs.append(norm_doc)\n",
        "\n",
        "    return norm_docs\n",
        "\n",
        "X_train['Clean Review'] = normalize_corpus(X_train['Review'].values)\n",
        "X_test['Clean Review'] = normalize_corpus(X_test['Review'].values)\n",
        "\n",
        "train_clean_text = X_train['Clean Review']\n",
        "test_clean_text = X_test['Clean Review']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vskus0X5oGdy"
      },
      "source": [
        "#### Train and Evaluate your BERT model using `transformers`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMBDMz8IGz2F"
      },
      "source": [
        "import transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-wzh9RyFqDj"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFWWTUR1FsB1"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}